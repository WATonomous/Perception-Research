{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafd5b89",
   "metadata": {},
   "source": [
    "# Carla Python API Tutorial\n",
    "From tutorial found [here](https://arijitray1993.github.io/CARLA_tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7953c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import random\n",
    "import cv2\n",
    "import skimage.measure as measure\n",
    "\n",
    "#in synchronous mode, sensor data must be added to a queue\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672117ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Game/Carla/Maps/Town03',\n",
       " '/Game/Carla/Maps/Town03_Opt',\n",
       " '/Game/Carla/Maps/Town10HD',\n",
       " '/Game/Carla/Maps/Town01_Opt',\n",
       " '/Game/Carla/Maps/Town05_Opt',\n",
       " '/Game/Carla/Maps/Town04',\n",
       " '/Game/Carla/Maps/Town10HD_Opt',\n",
       " '/Game/Carla/Maps/Town02_Opt',\n",
       " '/Game/Carla/Maps/Town02',\n",
       " '/Game/Carla/Maps/Town04_Opt',\n",
       " '/Game/Carla/Maps/Town05',\n",
       " '/Game/Carla/Maps/Town01']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = carla.Client('carla_server', 2000)\n",
    "client.set_timeout(60.0)\n",
    "\n",
    "client.get_available_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1dfd2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "time-out of 60000ms while waiting for the simulator, make sure the simulator is ready and connected to carla_server:2000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/1288721666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#camera and sensor data may not match simulation properly and will be noisy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronous_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: time-out of 60000ms while waiting for the simulator, make sure the simulator is ready and connected to carla_server:2000"
     ]
    }
   ],
   "source": [
    "world = client.load_world('Town04')\n",
    "settings = world.get_settings()\n",
    "settings.fixed_delta_seconds = 0.05 #must be less than 0.1, or else physics will be noisy\n",
    "#must use fixed delta seconds and synchronous mode for python api controlled sim, or else \n",
    "#camera and sensor data may not match simulation properly and will be noisy \n",
    "settings.synchronous_mode = True \n",
    "world.apply_settings(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ec873",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = carla.WeatherParameters(\n",
    "    cloudiness=20.0,\n",
    "    precipitation=20.0,\n",
    "    sun_altitude_angle=110.0)\n",
    "\n",
    "#or use precomputed weathers\n",
    "#weather = carla.WeatherParameters.WetCloudySunset\n",
    "\n",
    "world.set_weather(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61eba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "blueprints = world.get_blueprint_library().filter('*')\n",
    "for blueprint in random.sample(list(blueprints), 5):\n",
    "    print(blueprint.id)\n",
    "    for attr in blueprint:\n",
    "       print('  - {}'.format(attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175dd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282df70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blueprint_library = world.get_blueprint_library()\n",
    "bp = random.choice(blueprint_library.filter('vehicle')) # lets choose a vehicle at random\n",
    "\n",
    "# lets choose a random spawn point\n",
    "transform = random.choice(world.get_map().get_spawn_points()) \n",
    "\n",
    "#spawn a vehicle\n",
    "vehicle = world.spawn_actor(bp, transform) \n",
    "actor_list.append(vehicle)\n",
    "\n",
    "vehicle.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create waypoints for driving the vehicle around automatically\n",
    "m = world.get_map()\n",
    "waypoint = m.get_waypoint(transform.location)\n",
    "\n",
    "#lets add more vehicles\n",
    "for _ in range(0, 200):\n",
    "    transform = random.choice(m.get_spawn_points())\n",
    "\n",
    "    bp_vehicle = random.choice(blueprint_library.filter('vehicle'))\n",
    "\n",
    "    # This time we are using try_spawn_actor. If the spot is already\n",
    "    # occupied by another object, the function will return None.\n",
    "    other_vehicle = world.try_spawn_actor(bp_vehicle, transform)\n",
    "    if other_vehicle is not None:\n",
    "        #print(npc)\n",
    "        other_vehicle.set_autopilot(True)\n",
    "        actor_list.append(other_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding random objects\n",
    "blueprint_library = world.get_blueprint_library()\n",
    "weirdobj_bp = blueprint_library.find('static.prop.fountain')\n",
    "weirdobj_transform = random.choice(world.get_map().get_spawn_points())\n",
    "weirdobj_transform = carla.Transform(carla.Location(x=230, y=195, z=40), carla.Rotation(yaw=180))\n",
    "weird_obj = world.try_spawn_actor(weirdobj_bp, weirdobj_transform)\n",
    "actor_list.append(weird_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example for getting camera image\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=vehicle)\n",
    "image_queue = queue.Queue()\n",
    "camera.listen(image_queue.put)\n",
    "actor_list.append(camera)\n",
    "\n",
    "#example for getting depth camera image\n",
    "camera_depth = blueprint_library.find('sensor.camera.depth')\n",
    "camera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))\n",
    "camera_d = world.spawn_actor(camera_depth, camera_transform, attach_to=vehicle)\n",
    "image_queue_depth = queue.Queue()\n",
    "camera_d.listen(image_queue_depth.put)\n",
    "actor_list.append(camera_d)\n",
    "\n",
    "#example for getting semantic segmentation camera image\n",
    "camera_semseg = blueprint_library.find('sensor.camera.semantic_segmentation')\n",
    "camera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))\n",
    "camera_seg = world.spawn_actor(camera_semseg, camera_transform, attach_to=vehicle)\n",
    "image_queue_seg = queue.Queue()\n",
    "camera_seg.listen(image_queue_seg.put)\n",
    "actor_list.append(camera_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pycocotools\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def get_mask(seg_im, rgb_value):\n",
    "    # rgb_value should be somethiing like np.uint8([[[70, 70, 70]]])\n",
    "    # seg_im should be in HSV\n",
    "    \n",
    "    hsv_value = cv2.cvtColor(rgb_value, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    hsv_low = np.array([[[hsv_value[0][0][0]-5, hsv_value[0][0][1], hsv_value[0][0][2]-5]]])\n",
    "    hsv_high = np.array([[[hsv_value[0][0][0]+5, hsv_value[0][0][1], hsv_value[0][0][2]+5]]])\n",
    "    \n",
    "    mask = cv2.inRange(seg_im, hsv_low, hsv_high)\n",
    "    return mask\n",
    "\n",
    "def get_bbox_from_mask(mask):\n",
    "    label_mask = measure.label(mask)\n",
    "    props = measure.regionprops(label_mask)\n",
    "    \n",
    "    return [prop.bbox for prop in props]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get location of weird obj\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ee5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rgb camera\n",
    "image = image_queue.get()\n",
    "\n",
    "#semantic segmentation camera\n",
    "image_seg  = image_queue_seg.get()\n",
    "\n",
    "#depth camera\n",
    "image_depth = image_queue_depth.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3478f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save_to_disk(\"test_images/%06d.png\" %(image.frame))\n",
    "image_seg.save_to_disk(\"test_images/%06d_semseg.png\" %(image.frame), carla.ColorConverter.CityScapesPalette)\n",
    "image_depth.save_to_disk(\"test_images/%06d_depth.png\" %(image.frame), carla.ColorConverter.LogarithmicDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fa773",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread(\"test_images/%06d.png\" % image.frame)\n",
    "img_semseg = mpimg.imread(\"test_images/%06d_semseg.png\" % image.frame)\n",
    "img_depth = mpimg.imread(\"test_images/%06d_depth.png\" % image.frame)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12,18))\n",
    "ax1.imshow(img)\n",
    "ax2.imshow(img_semseg)\n",
    "ax3.imshow(img_depth) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_semseg_bgr = cv2.imread(\"test_images/%06d_semseg.png\" % image.frame)\n",
    "img_semseg_bgr = cv2.cvtColor(img_semseg_bgr, cv2.COLOR_BGRA2BGR)\n",
    "img_semseg_hsv = cv2.cvtColor(img_semseg_bgr, cv2.COLOR_BGR2HSV) # color wise segmentation is better in hsv space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgr value exmaples of few objects: full list at https://carla.readthedocs.io/en/0.9.9/ref_sensors/ \n",
    "object_list = {\n",
    "    'building': np.uint8([[[70, 70, 70]]]),\n",
    "    'pedestrian': np.uint8([[[220,  20,  60]]]),\n",
    "    'vegetation': np.uint8([[[107, 142,  35]]]),\n",
    "    'car': np.uint8([[[  0,   0, 142]]]),\n",
    "    'fence': np.uint8([[[190, 153, 153]]]),\n",
    "    'traffic_sign': np.uint8([[[220, 220,   0]]]),\n",
    "    'pole': np.uint8([[[153, 153, 153]]]),\n",
    "    'wall': np.uint8([[[102, 102, 156]]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_mask(img_semseg_hsv, object_list['car'])\n",
    "bboxes = get_bbox_from_mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb07e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,18))\n",
    "ax1.imshow(mask)\n",
    "for bbox in bboxes:\n",
    "    minr, minc, maxr, maxc = bbox\n",
    "    cv2.rectangle(img_semseg_bgr, (minc,minr), (maxc, maxr), (255,255,255), 6)\n",
    "\n",
    "ax2.imshow(img_semseg_bgr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f4c7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdobj_loc = weird_obj.get_location()\n",
    "# returns x, y, z as weirdobj_loc.x, weirdobj_loc.y, weirdobj_loc.z\n",
    "\n",
    "weirdobj_transform = weird_obj.get_transform()\n",
    "# returns x, y, z as weirdobj_transform.location.x, weirdobj_transform.location.y, weirdobj_transform.location.z\n",
    "# also returns pitch, yaw, roll as weirdobj_transform.rotation.pitch, weirdobj_transform.rotation.yaw, weirdobj_transform.rotation.roll\n",
    "\n",
    "#similarly we can get the camera transform\n",
    "camera_transform = camera.get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bcd7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "waypoint = random.choice(waypoint.next(1.5)) #navigate to next waypoint on map 1.5 meters ahead\n",
    "vehicle.set_transform(waypoint.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from detectron2.structures import BoxMode\n",
    "# #in sychronous mode, client controls step of simulation and number of steps\n",
    "# dataset_dicts = []\n",
    "# global_count=0\n",
    "# for i in range(100):\n",
    "#     #step\n",
    "#     world.tick()\n",
    "\n",
    "#     #rgb camera\n",
    "#     image = image_queue.get()\n",
    "\n",
    "#     #semantic segmentation camera\n",
    "#     image_seg  = image_queue_seg.get()\n",
    "#     #image_seg.convert(carla.ColorConverter.CityScapesPalette)\n",
    "\n",
    "#     #depth camera\n",
    "#     image_depth = image_queue_depth.get()\n",
    "#     #image_depth.convert(carla.ColorConverter.LogarithmicDepth)\n",
    "    \n",
    "    \n",
    "#     if i%10==0:\n",
    "#         image.save_to_disk(\"test_images/%06d.png\" %(image.frame))\n",
    "#         image_seg.save_to_disk(\"test_images/%06d_semseg.png\" %(image.frame), carla.ColorConverter.CityScapesPalette)\n",
    "#         image_depth.save_to_disk(\"test_images/%06d_depth.png\" %(image.frame), carla.ColorConverter.LogarithmicDepth)\n",
    "\n",
    "#         img = mpimg.imread(\"test_images/%06d.png\" % image.frame)\n",
    "#         img_semseg = mpimg.imread(\"test_images/%06d_semseg.png\" % image.frame)\n",
    "#         img_depth = mpimg.imread(\"test_images/%06d_depth.png\" % image.frame)\n",
    "        \n",
    "#         ## COCO format stuff, each image needs to have these keys\n",
    "#         height, width = cv2.imread(\"test_images/%06d.png\" %(image.frame)).shape[:2]\n",
    "#         record = {}\n",
    "#         record['file_name'] = \"test_images/%06d.png\" %(image.frame)\n",
    "#         global_count+=1\n",
    "#         record['image_id'] = global_count\n",
    "#         record['height'] = height\n",
    "#         record['width'] = width\n",
    "        \n",
    "        \n",
    "#         fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize = (12,18))\n",
    "#         ax1.imshow(img)\n",
    "#         ax2.imshow(img_semseg)\n",
    "#         ax3.imshow(img_depth) \n",
    "        \n",
    "        \n",
    "        \n",
    "#         ## compute bboxes from semseg\n",
    "#         img_semseg_bgr = cv2.imread(\"test_images/%06d_semseg.png\" % image.frame)\n",
    "#         img_semseg_bgr = cv2.cvtColor(img_semseg_bgr, cv2.COLOR_BGRA2BGR)\n",
    "#         img_semseg_hsv = cv2.cvtColor(img_semseg_bgr, cv2.COLOR_BGR2HSV) # color wise segmentation is better in hsv space\n",
    "\n",
    "#         #bgr value exmaples of few objects: full list at https://carla.readthedocs.io/en/0.9.9/ref_sensors/ \n",
    "#         object_list = dict()\n",
    "#         object_list['building'] = np.uint8([[[70, 70, 70]]])        \n",
    "#         object_list['pedestrian'] = np.uint8([[[220, 20, 60]]])\n",
    "#         object_list['vegetation'] = np.uint8([[[107, 142, 35]]])\n",
    "#         object_list['car'] = np.uint8([[[ 0, 0, 142]]])\n",
    "#         object_list['fence'] = np.uint8([[[ 190, 153, 153]]])\n",
    "#         object_list['traffic_sign'] = np.uint8([[[220, 220, 0]]])\n",
    "#         object_list['pole'] = np.uint8([[[153, 153, 153]]])\n",
    "#         object_list['wall'] = np.uint8([[[102, 102, 156]]])\n",
    "        \n",
    "#         object_bboxes = dict()\n",
    "#         objects = []\n",
    "#         obj_id = 0\n",
    "#         obj2id = dict()\n",
    "#         for obj in object_list:\n",
    "#             mask = get_mask(img_semseg_hsv, object_list[obj])\n",
    "#             bboxes = get_bbox_from_mask(mask)\n",
    "#             object_bboxes[obj] = bboxes\n",
    "            \n",
    "#             #let's visualize car bboxes\n",
    "#             if obj=='car':\n",
    "#                 ax4.imshow(mask)\n",
    "#                 for bbox in bboxes:\n",
    "#                     minr, minc, maxr, maxc = bbox\n",
    "#                     cv2.rectangle(img_semseg_bgr, (minc,minr), (maxc, maxr), (255,255,255), 6)\n",
    "        \n",
    "#                 ax5.imshow(img_semseg_bgr)\n",
    "            \n",
    "#             #lets put things in coco format for finetuning mask rcnn\n",
    "#             for bbox in bboxes:\n",
    "#                 minr, minc, maxr, maxc = bbox\n",
    "#                 obj_mask = np.copy(mask)\n",
    "#                 obj_mask[:minr] = 0\n",
    "#                 obj_mask[:, :minc] = 0\n",
    "#                 obj_mask[maxr+1:] = 0\n",
    "#                 obj_mask[:, maxc+1:] = 0\n",
    "\n",
    "#                 coco_rle_mask = pycocotools.mask.encode(np.array(obj_mask, order=\"F\"))\n",
    "                \n",
    "#                 obj_ann = {\n",
    "#                         'bbox': [minc, minr, maxc, maxr],\n",
    "#                         'bbox_mode': BoxMode.XYXY_ABS,\n",
    "#                         'segmentation': coco_rle_mask,\n",
    "#                         'category_id': obj_id\n",
    "#                 }\n",
    "#                 objects.append(obj_ann)\n",
    "                \n",
    "#                 obj_id+=1\n",
    "#                 obj2id[obj] = obj_id\n",
    "        \n",
    "        \n",
    "\n",
    "#         record['annotations'] = objects\n",
    "        \n",
    "#         print(record)\n",
    "        \n",
    "#         dataset_dicts.append(record)\n",
    "                \n",
    "            \n",
    "#         #plt.show()\n",
    "    \n",
    "#     #drive vehicle to next waypoint on map\n",
    "#     waypoint = random.choice(waypoint.next(1.5))\n",
    "#     vehicle.set_transform(waypoint.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f40a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to destroy all cameras and actors since they remain in the simulator even if you respawn using python. \n",
    "#It gets destroyed only if you restart CARLA simulator\n",
    "camera.destroy()\n",
    "camera_d.destroy()\n",
    "camera_seg.destroy()\n",
    "client.apply_batch([carla.command.DestroyActor(x) for x in actor_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a5a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
