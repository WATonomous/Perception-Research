version: "3.9"
services:
  centerpoint: 
    build:
      context: .
      dockerfile: docker/centerpoint/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/centerpoint

    ipc: host
    tty: true

    volumes:
      # TODO: Find out a way to mount source code without overwriting
      # the effect of running `python setup.py develop`
      - /mnt/wato-drive/perception/nuscenes_CLEAN:/CenterPoint/data/nuScenes

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]



  lss: 
    build:
      context: .
      dockerfile: docker/vm/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/vm

    ipc: host
    stdin_open: true
    tty: true

    volumes:
      - /mnt/wato-drive/perception/nuscenes_CLEAN:/nuscenes/trainval
      - /mnt/wato-drive/perception/nuscenes_CLEAN/maps:/nuscenes/maps
      - /mnt/scratch/lss:/lss
      - ./src/lift-splat-shoot:/lift-splat-shoot

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  foxglove: 
    image: ghcr.io/foxglove/studio:latest
    ipc: host
    tty: true

    ports:
      - "8082:8080"

    volumes:
      - /mnt/wato-drive/perception:/datasets

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  bevfusion: 
    build:
      context: .
      dockerfile: docker/bevfusion/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/bevfusion

    ipc: host
    tty: true

    volumes:
      # TODO: Find out a way to mount source code without overwriting
      # the effect of running `python setup.py develop`
      - ./src/bevfusion/.vscode:/root/bevfusion/.vscode
      - ./src/bevfusion/.gitignore:/root/bevfusion/.gitignore
      - ./src/bevfusion/tools:/root/bevfusion/tools
      # - ./src/bevfusion/mmdet3d:/root/bevfusion/mmdet3d # This breaks the code
      - /mnt/scratch/nuscenes_BEVFUSION/nuscenes_BEVFUSION:/root/bevfusion/data/nuscenes
      - /mnt/scratch/nuscenes_BEVFUSION/nuscenes_BEVFUSION:/drive_dataset
      - /mnt/scratch/bevfusion_pretrain:/pretrain

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  cuda-bevfusion: 
    build:
      context: .
      dockerfile: docker/cuda-bevfusion/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/cuda-bevfusion

    ipc: host
    tty: true

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  mmdetection3d: 
    build:
      context: .
      dockerfile: docker/mmdetection3d/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/mmdetection3d

    ipc: host
    stdin_open: true
    tty: true

    volumes:
      - ./:/Perception-Research

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]


  mmsegmentation: 
    build:
      context: .
      dockerfile: docker/mmsegmentation/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/mmsegmentation

    ipc: host
    stdin_open: true
    tty: true

    volumes:
      - ./src/models:/models

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]


  vm: 
    build:
      context: .
      dockerfile: docker/vm/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/vm

    ports:
      - "8887:8888"
    ipc: host
    stdin_open: true
    tty: true

    volumes:
      - ./:/Perception-Research

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]


  yolov5: 
    image: ultralytics/yolov5:latest

    ports:
      - "6006:6006"
    ipc: host
    stdin_open: true
    tty: true

    volumes: 
      - /mnt/wato-drive/perception:/datasets

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  
  carla_server:
    image: "carlasim/carla:0.9.13"
    runtime: nvidia
    command: /bin/bash -c "./CarlaUE4.sh -RenderOffScreen -quality-level=${CARLA_QUALITY:-Low} -world-port=2000 -nosound -carla-server"
    expose:
      - 2000
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  carla_dev:
    build:
      context: .
      dockerfile: docker/carla_dev/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/carla_dev

    runtime: nvidia
    command: /bin/bash -c "jupyter notebook --no-browser --allow-root --ip 0.0.0.0"

    ports:
      - "8886:8888"
    volumes:
      - ./src/carla:/home/carla

    ipc: host
    tty: True

    depends_on:
      - carla_server

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]


  pgp: 
    build:
      context: .
      dockerfile: docker/pgp/Dockerfile
    image: git.uwaterloo.ca:5050/watonomous/registry/perception-research/pgp-new-cuda

    ipc: host
    tty: true
    user: ${FIXUID:?}:${FIXGID:?}

    volumes:
      - ./src/PGP/:/home/docker/pgp/
      - /mnt/scratch/nuscenes_CLEAN:/nuscenes_CLEAN:ro
      - /mnt/scratch/nuscenes_PGP_PREPROCESS:/nuscenes_PGP_PREPROCESS
      - /mnt/scratch/PGP_lr-scheduler.tar:/PGP_lr-scheduler.tar:ro
      - /mnt/scratch/pgp_output:/pgp_output

    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0'] # Change to ['0', '1'] to use two GPUs
            capabilities: [gpu]
            